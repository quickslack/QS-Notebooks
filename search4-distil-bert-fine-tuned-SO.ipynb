{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# restart kernel after this\n",
    "!pip install wget annoy\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this in cli\n",
    "#pip install gpustat && watch -n 0.1 -c gpustat -cp --color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading in slack data and cleaning\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# for creating embeddings and index\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from annoy import AnnoyIndex\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# for downloading/extracting model from s3\n",
    "import tarfile\n",
    "import tempfile\n",
    "from transformers import cached_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze | grep numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_string = \"postgresql://postgres:postgres@postgres/postgres\"\n",
    "db = create_engine(db_string)\n",
    "\n",
    "def query_df(line_query, cell_query=None, conn=db):\n",
    "    if cell_query==None:\n",
    "      return pd.read_sql(line_query, conn)\n",
    "    return pd.read_sql(cell_query, conn)\n",
    "\n",
    "parent_query = 'SELECT * FROM message;'\n",
    "reply_query = 'SELECT * FROM reply;'\n",
    "\n",
    "parents = query_df(parent_query)\n",
    "replies = query_df(reply_query)\n",
    "\n",
    "df = pd.concat([parents, replies])\n",
    "df = df[['message_id', 'text']]\n",
    "assert df.isna().sum().sum() == 0\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_whitespace(text):\n",
    "    for r in ((\"\\t\", \" \"), (\"\\n\", \" \"), ('\"', '')):\n",
    "        text = text.replace(*r)\n",
    "    return text\n",
    "\n",
    "def no_url(text):\n",
    "    tokens = text.split()\n",
    "    new = []\n",
    "    for t in tokens:\n",
    "        if 'http' in t:\n",
    "            new.append('<URL>')        \n",
    "        else:\n",
    "            new.append(t)\n",
    "    clean = ' '.join(new)\n",
    "    return clean\n",
    "\n",
    "def no_short_reply(text):\n",
    "    if len(text) < 30:\n",
    "        text = None\n",
    "    return text\n",
    "\n",
    "def cleaner(series):\n",
    "    series = series.apply(no_whitespace)\n",
    "    series = series.apply(no_url)\n",
    "    series = series.apply(no_short_reply)\n",
    "    return series\n",
    "\n",
    "def fast_clean(df):\n",
    "    with Pool(16) as p:\n",
    "        seq = [df.text]\n",
    "        listy = p.map(cleaner, seq)\n",
    "        results = [pd.Series(i) for i in listy]\n",
    "        clean = results[0]\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['cleaned'] = fast_clean(df)\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Drop questions longer than 510 characters.\n",
    "df = df.loc[df['cleaned'].str.len() < 511]\n",
    "\n",
    "# Reset index.\n",
    "df = df.reset_index()\n",
    "\n",
    "# Get a list of all the posts/messages.\n",
    "corpus = list(df.cleaned)\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "url = 'https://model-2.s3.us-east-2.amazonaws.com/distil-bert-SO.tar.gz'\n",
    "\n",
    "def download_pretrained_model():\n",
    "    \"\"\" Download and extract finetuned model from S3 \"\"\"\n",
    "    # most of this func from https://github.com/huggingface/transfer-learning-conv-ai/blob/master/utils.py\n",
    "    resolved_archive_file = cached_path(url)\n",
    "    tempdir = tempfile.mkdtemp()\n",
    "    with tarfile.open(resolved_archive_file, 'r:gz') as archive:\n",
    "        archive.extractall(tempdir)\n",
    "    return os.path.join(tempdir, 'model')\n",
    "\n",
    "embedder = SentenceTransformer(download_pretrained_model())\n",
    "embedder.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "corpus_embeddings = embedder.encode(corpus)\n",
    "embs = np.asarray(corpus_embeddings)\n",
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# svd = TruncatedSVD(n_components=300)\n",
    "# reduced = svd.fit_transform(np.array(embs))\n",
    "# reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_docs, vec_dim = embs.shape\n",
    "distance_measure_type = 'angular'\n",
    "\n",
    "indx = AnnoyIndex(vec_dim, distance_measure_type)\n",
    "for i in range(num_docs):\n",
    "    indx.add_item(i, embs[i])\n",
    "\n",
    "num_trees = int(np.log(num_docs).round(0))\n",
    "# num_trees = 100\n",
    "print(num_trees)\n",
    "indx.build(num_trees)\n",
    "index_name = f'dim{vec_dim}-trees{num_trees}'\n",
    "index_file = f'{index_name}.ann'\n",
    "indx.save(index_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compress index\n",
    "# compressed_name = f'{index_name}.tar.gz'\n",
    "# command = f'tar czvf {compressed_name} {index_file}'\n",
    "# subprocess.check_output(command.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzvf dim768-trees13.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "index = AnnoyIndex(vec_dim, distance_measure_type)\n",
    "index.load(index_file)\n",
    "for i in index.get_nns_by_item(0,10):\n",
    "    print(i, df.cleaned[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ['tensorflow in production']\n",
    "emb = embedder.encode(example)\n",
    "# emb = svd.transform(np.array(emb))\n",
    "emb = np.asarray(emb)\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in index.get_nns_by_vector(emb.ravel(), 10): # Gets the top 5 similar to unseen example embedding\n",
    "#     print('\\n')\n",
    "    print(i, df.cleaned[i])\n",
    "\n",
    "# Search query: 'tensorflow in production'\n",
    "# Cherry-picked result from the above query w/ full sized embeddings:\n",
    "# '119220 Deploy machine learning models as web servers. <URL>'\n",
    "# None of the words are from the query, but the result has a similar meaning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
