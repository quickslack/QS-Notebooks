{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install scikit-learn\n",
    "!pip install gensim\n",
    "!pip install annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from annoy import AnnoyIndex\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_string = \"postgresql://postgres:postgres@postgres/postgres\"\n",
    "# db_string = \"postgresql://postgres:postgres@postgres/dev4slack\"\n",
    "db = create_engine(db_string)\n",
    "\n",
    "def query_df(line_query, cell_query=None, conn=db):\n",
    "    if cell_query==None:\n",
    "      return pd.read_sql(line_query, conn)\n",
    "    return pd.read_sql(cell_query, conn)\n",
    "\n",
    "# Custom notebook magic commands for loading sql.\n",
    "from IPython.core.magic import register_line_cell_magic\n",
    "def create_df_sql_magic(magic_name, conn):\n",
    "    def sql_df(line_query, cell_query=None, conn=db):\n",
    "        if cell_query==None:\n",
    "          return pd.read_sql(line_query, conn)\n",
    "        return pd.read_sql(cell_query, conn)\n",
    "    custom_func = sql_df\n",
    "    custom_func.__name__ = magic_name\n",
    "    register_line_cell_magic(custom_func)\n",
    "create_df_sql_magic('sql_df', db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT \n",
    "    message.text AS p, message.reply_count, message.user_id as p_id, message.ts,\n",
    "    reply.text AS c, reply.user_id as c_id\n",
    "FROM message\n",
    "LEFT JOIN reply on reply.thread_ts=message.ts\n",
    "WHERE message.channel_id='CFBBHV7AT' AND message.reply_count > 0\n",
    "ORDER BY message.ts, reply.ts;\n",
    "'''\n",
    "df = query_df(query)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['p', 'c']]\n",
    "df = pd.DataFrame(pd.concat([df.p, df.c]))\n",
    "df.columns = ['text']\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "df = df.dropna()\n",
    "assert df.isna().sum().sum() == 0\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_whitespace(text):\n",
    "    for r in ((\"\\t\", \" \"), (\"\\n\", \" \"), ('\"', '')):\n",
    "        text = text.replace(*r)\n",
    "    return text\n",
    "\n",
    "def no_url(text):\n",
    "    tokens = text.split()\n",
    "    new = []\n",
    "    for t in tokens:\n",
    "        if 'http' in t:\n",
    "            new.append('<URL>')\n",
    "        elif '<@' in t:\n",
    "            new.append('<USER>')\n",
    "        else:\n",
    "            new.append(t)\n",
    "    clean = ' '.join(new)\n",
    "    return clean\n",
    "\n",
    "def no_short_reply(text):\n",
    "    if len(text) < 10:\n",
    "        text = None\n",
    "    return text\n",
    "\n",
    "def cleaner(series):\n",
    "    series = series.apply(no_whitespace)\n",
    "    series = series.apply(no_url)\n",
    "    series = series.apply(no_short_reply)\n",
    "    return series\n",
    "\n",
    "def fast_clean(df):\n",
    "  # requires df to have columns 'p' and 'r' for parent and reply\n",
    "    with Pool(16) as p:\n",
    "        seq = [df[col] for col in list(df)]\n",
    "        listy = p.map(cleaner, seq)\n",
    "        results = [pd.Series(i) for i in listy]\n",
    "        clean = pd.concat(results, axis=1)\n",
    "        clean = clean.dropna()\n",
    "#         clean = clean[clean.p != clean.r]\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fast_clean(df)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Document retrieval w/ Tfidf vecs and cosine similarity\n",
    "\n",
    "v = TfidfVectorizer(stop_words='english')\n",
    "vecs = v.fit_transform(df.text)\n",
    "X = pd.DataFrame(vecs.todense(), columns=v.get_feature_names())\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "reduced = pca.fit_transform(vecs.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "example = ['what is the best way to deploy on heroku']\n",
    "ex_vec = v.transform(example)\n",
    "ex_vex = pca.transform(ex_vec.todense())\n",
    "cosine_similarities = ex_vec.dot(X.T)\n",
    "found = df.iloc[cosine_similarities.argmax()]\n",
    "print(found[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Doc retrieval w/ gensim doc2vec * tfidf. Approximate nearest neighbors w/ annoy\n",
    "\n",
    "num_cores = cpu_count()\n",
    "corpus = list(df.text)\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus)]\n",
    "model = Doc2Vec(vector_size=100, workers=num_cores, epochs=10)\n",
    "\n",
    "model.build_vocab(documents)\n",
    "model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))\n",
    "\n",
    "sigmoid_v = np.vectorize(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embeds = model.docvecs.vectors_docs\n",
    "\n",
    "r = sigmoid_v(reduced)\n",
    "e = sigmoid_v(embeds)\n",
    "\n",
    "combined = r * e\n",
    "num_docs, vec_dim = combined.shape\n",
    "\n",
    "indx = AnnoyIndex(vec_dim, 'angular')  #Length of item vector that will be indexed\n",
    "for i in range(num_docs):\n",
    "    indx.add_item(i, tfidf_times_embeds[i])\n",
    "\n",
    "trees = int(np.log(num_docs).round(0)) # just a rule of thumb\n",
    "print(trees)\n",
    "indx.build(trees)\n",
    "indx.save('a.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.min(), combined.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "index = AnnoyIndex(100, 'angular')\n",
    "index.load('a.ann')\n",
    "for i in index.get_nns_by_item(0,10): # Gets the top 10 similar to embedding @ index 0, including 0\n",
    "    print(df.text.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "example = ['what is the best way to deploy on heroku']\n",
    "embedding_vec = sigmoid_v(model.infer_vector(example))\n",
    "\n",
    "tfidf_vec = v.transform(example)\n",
    "reduced_vec = sigmoid_v(pca.transform(tfidf_vec.todense()))\n",
    "\n",
    "ex_vec = (reduced_vec * embedding_vec).ravel()\n",
    "ex_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in index.get_nns_by_vector(ex_vec, 5): # Gets the top 5 similar to unseen example embedding\n",
    "    print('\\n')\n",
    "    print(i, df.text.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
