{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install scikit-learn transformers annoy\n",
    "!pip install ipywidgets\n",
    "# !pip install --upgrade jupyter_client\n",
    "\n",
    "# from ipywidgets import IntProgress\n",
    "\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from sklearn.decomposition import PCA\n",
    "from annoy import AnnoyIndex\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "db_string = \"postgresql://postgres:postgres@postgres/postgres\"\n",
    "db = create_engine(db_string)\n",
    "\n",
    "def query_df(line_query, cell_query=None, conn=db):\n",
    "    if cell_query==None:\n",
    "      return pd.read_sql(line_query, conn)\n",
    "    return pd.read_sql(cell_query, conn)\n",
    "\n",
    "# Custom notebook magic commands for loading sql.\n",
    "from IPython.core.magic import register_line_cell_magic\n",
    "def create_df_sql_magic(magic_name, conn):\n",
    "    def sql_df(line_query, cell_query=None, conn=db):\n",
    "        if cell_query==None:\n",
    "          return pd.read_sql(line_query, conn)\n",
    "        return pd.read_sql(cell_query, conn)\n",
    "    custom_func = sql_df\n",
    "    custom_func.__name__ = magic_name\n",
    "    register_line_cell_magic(custom_func)\n",
    "create_df_sql_magic('sql_df', db)\n",
    "\n",
    "parent_query = 'SELECT * FROM message;'\n",
    "reply_query = 'SELECT * FROM reply;'\n",
    "\n",
    "parents = query_df(parent_query)\n",
    "replies = query_df(reply_query)\n",
    "\n",
    "df = pd.concat([parents, replies])\n",
    "df = df[['message_id', 'text']]\n",
    "assert df.isna().sum().sum() == 0\n",
    "print(df.shape)\n",
    "\n",
    "def no_whitespace(text):\n",
    "    for r in ((\"\\t\", \" \"), (\"\\n\", \" \"), ('\"', '')):\n",
    "        text = text.replace(*r)\n",
    "    return text\n",
    "\n",
    "def no_short_reply(text):\n",
    "    if len(text) < 10:\n",
    "        text = None\n",
    "    return text\n",
    "\n",
    "def cleaner(series):\n",
    "    series = series.apply(no_whitespace)\n",
    "    series = series.apply(no_short_reply)\n",
    "    return series\n",
    "\n",
    "def fast_clean(df):\n",
    "    with Pool(16) as p:\n",
    "        seq = [df.text]\n",
    "        listy = p.map(cleaner, seq)\n",
    "        results = [pd.Series(i) for i in listy]\n",
    "        clean = results[0]\n",
    "    return clean\n",
    "\n",
    "%%time\n",
    "df['cleaned'] = fast_clean(df)\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# model.eval();\n",
    "\n",
    "# # If you have a GPU, put everything on cuda\n",
    "# tokens_tensor = tokens_tensor.to('cuda')\n",
    "# segments_tensors = segments_tensors.to('cuda')\n",
    "# model.to('cuda')\n",
    "\n",
    "model.to('cuda');\n",
    "sample = df.sample(10000)\n",
    "\n",
    "embeddings = []\n",
    "for i in sample.cleaned:\n",
    "    input_ids = torch.tensor(tokenizer.encode([i])).unsqueeze(0)\n",
    "    input_ids = input_ids.to('cuda')\n",
    "    outputs = model(input_ids)\n",
    "    emb = outputs[0]\n",
    "    np_emb = emb.cpu().detach().numpy()\n",
    "    embeddings.append(np_emb.flatten())\n",
    "\n",
    "%%time\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=300)\n",
    "reduced = svd.fit_transform(np.array(embeddings))\n",
    "reduced.shape\n",
    "\n",
    "%%time\n",
    "num_docs, vec_dim = reduced.shape\n",
    "\n",
    "indx = AnnoyIndex(vec_dim, 'angular')\n",
    "for i in range(num_docs):\n",
    "    indx.add_item(i, reduced[i])\n",
    "\n",
    "trees = int(np.log(num_docs).round(0))\n",
    "print(trees)\n",
    "indx.build(trees)\n",
    "indx.save('berty.ann')\n",
    "\n",
    "%%time\n",
    "index = AnnoyIndex(300, 'angular')\n",
    "index.load('berty.ann')\n",
    "for i in index.get_nns_by_item(0,10):\n",
    "    print(i, sample.cleaned.iloc[i])\n",
    "    \n",
    "%%time\n",
    "example = ['how do I deploy to heroku']\n",
    "\n",
    "input_ids = torch.tensor(tokenizer.encode(example)).unsqueeze(0)\n",
    "input_ids = input_ids.to('cuda')\n",
    "outputs = model(input_ids)\n",
    "emb = outputs[0]\n",
    "np_emb = emb.cpu().detach().numpy()\n",
    "vec = np_emb.flatten()\n",
    "vec = svd.transform([vec])\n",
    "vec.shape\n",
    "\n",
    "%%time\n",
    "for i in index.get_nns_by_vector(vec.ravel(), 10):\n",
    "    print(i, df.cleaned.iloc[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
