{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_string = \"postgresql://postgres:postgres@postgres/postgres\"\n",
    "# db_string = \"postgresql://postgres:postgres@postgres/dev4slack\"\n",
    "db = create_engine(db_string)\n",
    "\n",
    "def query_df(line_query, cell_query=None, conn=db):\n",
    "    if cell_query==None:\n",
    "      return pd.read_sql(line_query, conn)\n",
    "    return pd.read_sql(cell_query, conn)\n",
    "\n",
    "# Custom notebook magic commands for loading sql.\n",
    "from IPython.core.magic import register_line_cell_magic\n",
    "def create_df_sql_magic(magic_name, conn):\n",
    "    def sql_df(line_query, cell_query=None, conn=db):\n",
    "        if cell_query==None:\n",
    "          return pd.read_sql(line_query, conn)\n",
    "        return pd.read_sql(cell_query, conn)\n",
    "    custom_func = sql_df\n",
    "    custom_func.__name__ = magic_name\n",
    "    register_line_cell_magic(custom_func)\n",
    "create_df_sql_magic('sql_df', db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \\\n",
    "'''\n",
    "SELECT \n",
    "    message.text AS p_text, message.reply_count, message.user_id as p_id, message.ts,\n",
    "    reply.text AS c_text, reply.user_id as c_id\n",
    "FROM message\n",
    "LEFT JOIN reply on reply.thread_ts=message.ts\n",
    "WHERE message.channel_id='CFBBHV7AT' AND message.reply_count > 0\n",
    "ORDER BY message.ts, reply.ts;\n",
    "'''\n",
    "df = query_df(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column of replies to each element in c_text.\n",
    "df['replies'] = df.c_text.shift(-1)\n",
    "\n",
    "# Drop rows where a reply refers to an unrelated parent.\n",
    "df = df.groupby('ts', as_index=False).apply(lambda x: x.iloc[:-1])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_clean(col):\n",
    "    '''replaces whitespace, quotes, and urls'''\n",
    "    col.replace({'\\t':' ','\\n':' ','\"':''},\n",
    "#                 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+':'URL>'},\n",
    "                inplace=True, regex=True)\n",
    "\n",
    "cols2clean = ['c_text', 'replies']\n",
    "for col in cols2clean:\n",
    "    simple_clean(df[col])\n",
    "\n",
    "# Drop rows with duplicate column items.\n",
    "df = df[df.c_text != df.replies]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta(url, text):\n",
    "    '''Tries to get a url meta description.'''\n",
    "    response = requests.get(url)\n",
    "#     print(response.status_code)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text)\n",
    "        metas = soup.find_all('meta')\n",
    "        meta_description = [meta.attrs['content'] for meta in metas \\\n",
    "                            if 'name' in meta.attrs and meta.attrs['name'] == 'description']\n",
    "        if meta_description[0]:\n",
    "#             print(meta_description)\n",
    "            before_url = 'here is a link: '\n",
    "            url_meta = f' it is about, and I quote, \"{meta_description[0]}\"'\n",
    "            cleaned = text.replace(f'<{url}>', f'{before_url}{url}{url_meta}')\n",
    "            return cleaned\n",
    "        else:\n",
    "            nope = text.replace(f'<{url}>', '<URL>')\n",
    "            return nope\n",
    "    else:\n",
    "        nope = text.replace(f'<{url}>', '<DEAD_URL>')\n",
    "        return nope\n",
    "    \n",
    "\n",
    "def describe_urls(text):\n",
    "    '''Looks for urls in text. Replaces urls with their scraped meta description.'''\n",
    "    pat = '<((?!@).*)>'\n",
    "    pat_found = re.search(f'{pat}', text)\n",
    "    if pat_found:\n",
    "#         print('was found')\n",
    "        url = pat_found.group(0)\n",
    "        # take off the \"<>\"\n",
    "        url = url[1:-1]\n",
    "#         print(url)\n",
    "        if \"|\" in url:\n",
    "#             print('removed |')\n",
    "            # remove anything after \"|\"\n",
    "            url = re.sub('\\|.*$','', url)\n",
    "        try:\n",
    "#             print('trying get_meta')\n",
    "            cleaned = get_meta(url, text)\n",
    "#             print('tried')\n",
    "            return cleaned\n",
    "        except:\n",
    "            return re.sub(rf'<({url})>', '<URL>', text)\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_test = 'Considerable twitter clout to anyone who has a good technical solution: <https://meta.stackoverflow.com/questions/293750/are-sites-that-autonomously-scrape-stack-overflow-for-answers-to-programming-pro>'\n",
    "describe_urls(url_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['replies'] = df['replies'].apply(describe_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(100)\n",
    "pat = '<((?!@).*)>'\n",
    "r_urls = sample.replies.str.contains(rf'{pat}').sum()\n",
    "c_urls = sample.c_text.str.contains(rf'{pat}').sum()\n",
    "print(f'total urls: {r_urls + c_urls}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['replies'] = sample['replies'].apply(describe_urls)\n",
    "sample['c_text'] = sample['c_text'].apply(describe_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_nope = sample.replies.str.contains('<URL>').sum()\n",
    "c_nope = sample.c_text.str.contains('<URL>').sum()\n",
    "r_found = sample.replies.str.contains(rf'{pat}').sum()\n",
    "c_found = sample.c_text.str.contains(rf'{pat}').sum()\n",
    "print(\n",
    "    f'''\n",
    "    total descriptions found: {r_found + c_found}\n",
    "    total not found: {r_nope + c_nope}\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(100)\n",
    "pat = '<((?!@).*)>'\n",
    "r_urls = sample.replies.str.contains(rf'{pat}').sum()\n",
    "c_urls = sample.c_text.str.contains(rf'{pat}').sum()\n",
    "print(f'total urls: {r_urls + c_urls}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = []\n",
    "for i in range(99):\n",
    "    text = sample.c_text.iloc[i]\n",
    "    pat_found = re.search(f'{pat}', text)\n",
    "    if pat_found:\n",
    "        test_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "\n",
    "with Pool(32) as p:\n",
    "     data_list = list(p.imap_unordered(describe_urls, test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get text with \"<@user>\" formatted as replies to that user.\n",
    "# ats = df[df.replies.str.contains('<@')]\n",
    "# ats.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
