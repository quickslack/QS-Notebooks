{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!git clone https://github.com/microsoft/DialoGPT.git dgpt\n",
    "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" git+https://github.com/NVIDIA/apex.git@3d01e4a0a188cc8df54bc6e44cf5eb40ff6b4cc5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/QS-Notebooks/owen/QS-Notebooks/dgpt\n",
      "PROJECT_FOLDER = /notebooks/QS-Notebooks/owen/QS-Notebooks/dgpt\n",
      "Found existing models folder at /notebooks/QS-Notebooks/owen/QS-Notebooks/dgpt/models, skip creating a new one!\n",
      "02/08/2020 00:21:32 - INFO - __main__ -   Downloading models...\n",
      "02/08/2020 00:21:32 - INFO - demo_utils -   /notebooks/QS-Notebooks/owen/QS-Notebooks/dgpt/models/medium/config.json exists, return!\n",
      "02/08/2020 00:21:32 - INFO - demo_utils -   /notebooks/QS-Notebooks/owen/QS-Notebooks/dgpt/models/medium/vocab.json exists, return!\n",
      "02/08/2020 00:21:32 - INFO - demo_utils -   /notebooks/QS-Notebooks/owen/QS-Notebooks/dgpt/models/medium/merges.txt exists, return!\n",
      "02/08/2020 00:21:32 - INFO - demo_utils -   /notebooks/QS-Notebooks/owen/QS-Notebooks/dgpt/models/medium/pytorch_model.bin exists, return!\n",
      "02/08/2020 00:21:32 - INFO - demo_utils -   /notebooks/QS-Notebooks/owen/QS-Notebooks/dgpt/models/medium/medium_ft.pkl exists, return!\n",
      "02/08/2020 00:21:32 - INFO - __main__ -   Done!\n",
      "\n",
      "02/08/2020 00:21:32 - INFO - __main__ -   Downloading and Extracting Data...\n",
      "02/08/2020 00:21:32 - INFO - __main__ -   Preparing Data...\n",
      "/notebooks/QS-Notebooks/owen/QS-Notebooks/dgpt/data/train.128len.db exists, skip prepro.py\n",
      "02/08/2020 00:21:32 - INFO - __main__ -   Done!\n",
      "\n",
      "02/08/2020 00:21:32 - INFO - __main__ -   Generating training CMD!\n",
      "02/08/2020 00:21:32 - INFO - __main__ -   If there is any problem, please copy (modify) and run command below\n",
      "02/08/2020 00:21:32 - INFO - __main__ -   #########################################################################\n",
      "python LSP_train.py --model_name_or_path /notebooks/QS-Notebooks/owen/QS-Notebooks/dgpt/models/medium --init_checkpoint /notebooks/QS-Notebooks/owen/QS-Notebooks/dgpt/models/medium/pytorch_model.bin --train_input_file /notebooks/QS-Notebooks/owen/QS-Notebooks/dgpt/data/train.128len.db --eval_input_file ./data/test.tsv --output_dir /notebooks/QS-Notebooks/owen/QS-Notebooks/dgpt/models/output_model --seed 42 --max_seq_length 128 --train_batch_size 128 --gradient_accumulation_steps 8 --eval_batch_size 64 --learning_rate 1e-5 --num_optim_steps 10000 --valid_step 5000 --warmup_steps 4000 --normalize_data true --fp16 true --lr_schedule noam --loss_scale 0.0 --no_token_id true --pbar true\n",
      "02/08/2020 00:21:32 - INFO - __main__ -   #########################################################################\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   train batch size = 128, new train batch size (after gradient accumulation) = 16\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   CUDA available? True\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   Input Argument Information\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   model_name_or_path            /notebooks/QS-Notebooks/owen/QS-Notebooks/dgpt/models/medium\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   seed                          42\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   max_seq_length                128\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   skip_eval                     False\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   init_checkpoint               /notebooks/QS-Notebooks/owen/QS-Notebooks/dgpt/models/medium/pytorch_model.bin\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   train_input_file              /notebooks/QS-Notebooks/owen/QS-Notebooks/dgpt/data/train.128len.db\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   eval_input_file               ./data/test.tsv\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   continue_from                 0\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   train_batch_size              16\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   gradient_accumulation_steps   8\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   eval_batch_size               64\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   learning_rate                 1e-05\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   num_optim_steps               10000\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   valid_step                    5000\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   warmup_proportion             0.1\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   warmup_steps                  4000\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   normalize_data                True\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   fp16                          True\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   lr_schedule                   noam\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   loss_scale                    0.0\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   no_token_id                   True\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   output_dir                    /notebooks/QS-Notebooks/owen/QS-Notebooks/dgpt/models/output_model\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   log_dir                       None\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   pbar                          True\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   local_rank                    -1\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   config                        None\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   device                        cuda\n",
      "02/08/2020 00:21:33 - INFO - __main__ -   n_gpu                         1\n",
      "02/08/2020 00:21:33 - INFO - pytorch_pretrained_bert.tokenization_gpt2 -   loading vocabulary file /notebooks/QS-Notebooks/owen/QS-Notebooks/dgpt/models/medium/vocab.json\n",
      "02/08/2020 00:21:33 - INFO - pytorch_pretrained_bert.tokenization_gpt2 -   loading merges file /notebooks/QS-Notebooks/owen/QS-Notebooks/dgpt/models/medium/merges.txt\n",
      "02/08/2020 00:22:22 - INFO - gpt2_training.train_utils -   loading finetuned model from /notebooks/QS-Notebooks/owen/QS-Notebooks/dgpt/models/medium/pytorch_model.bin\n",
      "02/08/2020 00:22:22 - INFO - gpt2_training.train_utils -   loading transfomer only\n",
      "02/08/2020 00:22:23 - INFO - gpt2_training.train_utils -   in fp16, model.half() activated\n",
      "02/08/2020 00:22:31 - INFO - __main__ -   Number of parameter = 354823168\n",
      "02/08/2020 00:22:31 - INFO - __main__ -   in fp16, using FusedAdam\n",
      "training:   3%|3         | 325/10000 [05:06<2:36:14,  1.03it/s, tok/s: 7.0k ppl: 61.33 epoch: 0]Traceback (most recent call last):\n",
      "  File \"LSP_train.py\", line 288, in <module>\n",
      "    optimizer.backward(loss)\n",
      "  File \"/home/nbserver/.local/lib/python3.6/site-packages/apex/optimizers/fp16_optimizer.py\", line 172, in backward\n",
      "    scaled_loss.backward()\n",
      "  File \"/home/nbserver/.local/lib/python3.6/site-packages/torch/tensor.py\", line 195, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/nbserver/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 99, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 198.00 MiB (GPU 0; 10.76 GiB total capacity; 9.21 GiB already allocated; 182.25 MiB free; 9.73 GiB reserved in total by PyTorch)\n",
      "training:   3%|3         | 325/10000 [05:06<2:32:17,  1.06it/s, tok/s: 7.0k ppl: 61.33 epoch: 0]\n",
      "02/08/2020 00:27:39 - INFO - __main__ -   Done!\n",
      "\n",
      "368.60999059677124\n"
     ]
    }
   ],
   "source": [
    "# if 'no module named' error, run cells below\n",
    "now = time.time()\n",
    "!cd dgpt && pwd && python3 demo.py --data small\n",
    "print(time.time() - now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
