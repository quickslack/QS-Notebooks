{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from annoy import AnnoyIndex\n",
    "import pickle\n",
    "\n",
    "db_string = \"postgresql://postgres:postgres@postgres/postgres\"\n",
    "db = create_engine(db_string)\n",
    "\n",
    "def query_df(line_query, cell_query=None, conn=db):\n",
    "    if cell_query==None:\n",
    "      return pd.read_sql(line_query, conn)\n",
    "    return pd.read_sql(cell_query, conn)\n",
    "\n",
    "# Custom notebook magic commands for loading sql.\n",
    "from IPython.core.magic import register_line_cell_magic\n",
    "def create_df_sql_magic(magic_name, conn):\n",
    "    def sql_df(line_query, cell_query=None, conn=db):\n",
    "        if cell_query==None:\n",
    "          return pd.read_sql(line_query, conn)\n",
    "        return pd.read_sql(cell_query, conn)\n",
    "    custom_func = sql_df\n",
    "    custom_func.__name__ = magic_name\n",
    "    register_line_cell_magic(custom_func)\n",
    "create_df_sql_magic('sql_df', db)\n",
    "\n",
    "parent_query = 'SELECT * FROM message;'\n",
    "reply_query = 'SELECT * FROM reply;'\n",
    "\n",
    "parents = query_df(parent_query)\n",
    "replies = query_df(reply_query)\n",
    "\n",
    "df = pd.concat([parents, replies])\n",
    "df = df[['message_id', 'text']]\n",
    "assert df.isna().sum().sum() == 0\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_whitespace(text):\n",
    "    for r in ((\"\\t\", \" \"), (\"\\n\", \" \"), ('\"', '')):\n",
    "        text = text.replace(*r)\n",
    "    return text\n",
    "\n",
    "def no_short_reply(text):\n",
    "    if len(text) < 10:\n",
    "        text = None\n",
    "    return text\n",
    "\n",
    "def cleaner(series):\n",
    "    series = series.apply(no_whitespace)\n",
    "    series = series.apply(no_short_reply)\n",
    "    return series\n",
    "\n",
    "def fast_clean(df):\n",
    "    with Pool(16) as p:\n",
    "        seq = [df.text]\n",
    "        listy = p.map(cleaner, seq)\n",
    "        results = [pd.Series(i) for i in listy]\n",
    "        clean = results[0]\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['cleaned'] = fast_clean(df)\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "vecs = tfidf.fit_transform(df.cleaned)\n",
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "reduced = svd.fit_transform(vecs)\n",
    "reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_docs, vec_dim = reduced.shape\n",
    "\n",
    "indx = AnnoyIndex(vec_dim, 'angular')\n",
    "for i in range(num_docs):\n",
    "    indx.add_item(i, reduced[i])\n",
    "\n",
    "trees = int(np.log(num_docs).round(0))\n",
    "print(trees)\n",
    "indx.build(trees)\n",
    "indx.save('annoy.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "index = AnnoyIndex(100, 'angular')\n",
    "index.load('annoy.ann')\n",
    "for i in index.get_nns_by_item(0,10):\n",
    "    print(i, df.cleaned[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "example = ['heroku']\n",
    "vec = tfidf.transform(example)\n",
    "vec = svd.transform(vec)\n",
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in index.get_nns_by_vector(vec.ravel(), 10): # Gets the top 5 similar to unseen example embedding\n",
    "#     print('\\n')\n",
    "    print(i, df.cleaned[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['message_id']]\n",
    "df.to_csv('message_ids.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile insert.py\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "db_string = \"postgresql://postgres:postgres@postgres/postgres\"\n",
    "db = create_engine(db_string)\n",
    "\n",
    "df = pd.read_csv('cleaned.csv')\n",
    "\n",
    "conn = db.raw_connection()\n",
    "cur = conn.cursor()\n",
    "\n",
    "for _, row in tqdm(df.iterrows()):\n",
    "    cur.execute('INSERT INTO cleaned VALUES(%s, %s)', (row['message_id'], row['cleaned']))\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -ahl a.ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tfidf, open(\"tfidf.pkl\", \"wb\"))\n",
    "pickle.dump(svd, open(\"svd.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "target = 'tfidf.pkl'\n",
    "\n",
    "if os.path.getsize(target) > 0:      \n",
    "    with open(target, \"rb\") as f:\n",
    "        unpickler = pickle.Unpickler(f)\n",
    "        test = unpickler.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = test.transform(['blah blah'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(r'cleaned.txt', df.values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
